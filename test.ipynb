{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import net\n",
    "from function import adaptive_instance_normalization, coral\n",
    "\n",
    "\n",
    "def test_transform(size, crop):\n",
    "    transform_list = []\n",
    "    if size != 0:\n",
    "        transform_list.append(transforms.Resize(size))\n",
    "    if crop:\n",
    "        transform_list.append(transforms.CenterCrop(size))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "\n",
    "def style_transfer(vgg, decoder, content, style, alpha=1.0,\n",
    "                   interpolation_weights=None):\n",
    "    assert (0.0 <= alpha <= 1.0)\n",
    "    content_f = vgg(content)\n",
    "    style_f = vgg(style)\n",
    "    if interpolation_weights:\n",
    "        _, C, H, W = content_f.size()\n",
    "        feat = torch.FloatTensor(1, C, H, W).zero_().to(device)\n",
    "        base_feat = adaptive_instance_normalization(content_f, style_f)\n",
    "        for i, w in enumerate(interpolation_weights):\n",
    "            feat = feat + w * base_feat[i:i + 1]\n",
    "        content_f = content_f[0:1]\n",
    "    else:\n",
    "        feat = adaptive_instance_normalization(content_f, style_f)\n",
    "    feat = feat * alpha + content_f * (1 - alpha)\n",
    "    return decoder(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "do_interpolation = False\n",
    "\n",
    "content_paths = [Path('content/serega.jpg')]\n",
    "style_paths = [Path('style/sketch.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = net.decoder\n",
    "vgg = net.vgg\n",
    "\n",
    "decoder.eval()\n",
    "vgg.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load('models/decoder.pth'))\n",
    "vgg.load_state_dict(torch.load('models/vgg_normalised.pth'))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "vgg.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "content_size = 1024\n",
    "style_size = 500\n",
    "crop = True\n",
    "preserve_color = False\n",
    "alpha = 1.0\n",
    "interpolation_weights = [0.3, 0.7]\n",
    "\n",
    "content_tf = test_transform(content_size, crop)\n",
    "style_tf = test_transform(style_size, crop)\n",
    "\n",
    "for content_path in content_paths:\n",
    "    if do_interpolation:  # one content image, N style image\n",
    "        style = torch.stack([style_tf(Image.open(str(p))) for p in style_paths])\n",
    "        content = content_tf(Image.open(str(content_path))) \\\n",
    "            .unsqueeze(0).expand_as(style)\n",
    "        style = style.to(device)\n",
    "        content = content.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = style_transfer(vgg, decoder, content, style,\n",
    "                                    alpha, interpolation_weights)\n",
    "        output = output.cpu()\n",
    "        output_name = output_dir / '{:s}_interpolation_alpha_{:s}{:s}'.format(\n",
    "            content_path.stem, str(alpha), '.jpg')\n",
    "        save_image(output, str(output_name))\n",
    "\n",
    "    else:  # process one content and one style\n",
    "        for style_path in style_paths:\n",
    "            content = content_tf(Image.open(str(content_path)))\n",
    "            style = style_tf(Image.open(str(style_path)))\n",
    "            if preserve_color:\n",
    "                style = coral(style, content)\n",
    "            style = style.to(device).unsqueeze(0)\n",
    "            content = content.to(device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                output = style_transfer(vgg, decoder, content, style,\n",
    "                                        alpha)\n",
    "            output = output.cpu()\n",
    "\n",
    "            output_name = output_dir / '{:s}_stylized_{:s}_alpha_{:s}{:s}'.format(\n",
    "                content_path.stem, style_path.stem, str(alpha), '.jpg')\n",
    "            save_image(output, str(output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(content_paths, style_paths):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    output_dir = Path('output')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    do_interpolation = False\n",
    "    decoder = net.decoder\n",
    "    vgg = net.vgg\n",
    "\n",
    "    decoder.eval()\n",
    "    vgg.eval()\n",
    "\n",
    "    decoder.load_state_dict(torch.load('models/decoder.pth'))\n",
    "    vgg.load_state_dict(torch.load('models/vgg_normalised.pth'))\n",
    "    vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "    vgg.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    content_size = 1024\n",
    "    style_size = 500\n",
    "    crop = True\n",
    "    preserve_color = False\n",
    "    alpha = 1.0\n",
    "    interpolation_weights = [0.3, 0.7]\n",
    "\n",
    "    content_tf = test_transform(content_size, crop)\n",
    "    style_tf = test_transform(style_size, crop)\n",
    "\n",
    "    for content_path in content_paths:\n",
    "        if do_interpolation:  # one content image, N style image\n",
    "            style = torch.stack([style_tf(Image.open(str(p))) for p in style_paths])\n",
    "            content = content_tf(Image.open(str(content_path))) \\\n",
    "                .unsqueeze(0).expand_as(style)\n",
    "            style = style.to(device)\n",
    "            content = content.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = style_transfer(vgg, decoder, content, style,\n",
    "                                        alpha, interpolation_weights)\n",
    "            output = output.cpu()\n",
    "            output_name = output_dir / '{:s}_interpolation_alpha_{:s}{:s}'.format(\n",
    "                content_path.stem, str(alpha), '.jpg')\n",
    "            save_image(output, str(output_name))\n",
    "\n",
    "        else:  # process one content and one style\n",
    "            for style_path in style_paths:\n",
    "                content = content_tf(Image.open(str(content_path)))\n",
    "                style = style_tf(Image.open(str(style_path)))\n",
    "                if preserve_color:\n",
    "                    style = coral(style, content)\n",
    "                style = style.to(device).unsqueeze(0)\n",
    "                content = content.to(device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    output = style_transfer(vgg, decoder, content, style,\n",
    "                                            alpha)\n",
    "                output = output.cpu()\n",
    "\n",
    "                output_name = output_dir / '{:s}_stylized_{:s}_alpha_{:s}{:s}'.format(\n",
    "                    content_path.stem, style_path.stem, str(alpha), '.jpg')\n",
    "                save_image(output, str(output_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
